<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · SDDP.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>SDDP.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><span class="toctext">Tutorials</span><ul><li><a class="toctext" href="../tutorial/01_first_steps/">Tutorial One: first steps</a></li><li><a class="toctext" href="../tutorial/02_adding_uncertainty/">Tutorial Two: adding uncertainty</a></li><li><a class="toctext" href="../tutorial/03_objective_uncertainty/">Tutorial Three: objective uncertainty</a></li><li><a class="toctext" href="../tutorial/04_markov_uncertainty/">Tutorial Four: Markov uncertainty</a></li></ul></li><li class="current"><a class="toctext" href>Reference</a><ul class="internal"><li><a class="toctext" href="#Policy-graphs-1">Policy graphs</a></li><li><a class="toctext" href="#Subproblem-definition-1">Subproblem definition</a></li><li><a class="toctext" href="#Training-the-policy-1">Training the policy</a></li><li><a class="toctext" href="#Simulating-the-policy-1">Simulating the policy</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Reference</a></li></ul><a class="edit-page" href="https://github.com/odow/Kokako.jl/blob/master/docs/src/apireference.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Reference</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="API-Reference-1" href="#API-Reference-1">API Reference</a></h1><h2><a class="nav-anchor" id="Policy-graphs-1" href="#Policy-graphs-1">Policy graphs</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Kokako.LinearPolicyGraph" href="#Kokako.LinearPolicyGraph"><code>Kokako.LinearPolicyGraph</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">LinearPolicyGraph(builder::Function; stages::Int, kwargs...)</code></pre><p>Create a linear policy graph with <code>stages</code> number of stages.</p><p>See <a href="#Kokako.PolicyGraph"><code>PolicyGraph</code></a> for the other keyword arguments.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Kokako.MarkovianPolicyGraph" href="#Kokako.MarkovianPolicyGraph"><code>Kokako.MarkovianPolicyGraph</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">MarkovianPolicyGraph(builder::Function;
    transition_matrices::Vector{Array{Float64, 2}}, kwargs...)</code></pre><p>Create a Markovian policy graph based on the transition matrices given in <code>transition_matrices</code>.</p><p>See <a href="#Kokako.PolicyGraph"><code>PolicyGraph</code></a> for the other keyword arguments.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Kokako.PolicyGraph" href="#Kokako.PolicyGraph"><code>Kokako.PolicyGraph</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">PolicyGraph(builder::Function, graph::Graph{T};
            bellman_function = AverageCut,
            optimizer = nothing,
            direct_mode = true) where T</code></pre><p>Construct a a policy graph based on the graph structure of <code>graph</code>. (See <code>Graph</code> for details.)</p><p><strong>Example</strong></p><pre><code class="language-none">function builder(subproblem::JuMP.Model, index)
    # ... subproblem definition ...
end
model = PolicyGraph(builder, graph;
                    bellman_function = AverageCut,
                    optimizer = with_optimizer(GLPK.Optimizer),
                    direct_mode = false)</code></pre><p>Or, using the Julia <code>do ... end</code> syntax:</p><pre><code class="language-none">model = PolicyGraph(graph;
                    bellman_function = AverageCut,
                    optimizer = with_optimizer(GLPK.Optimizer),
                    direct_mode = true) do subproblem, index
    # ... subproblem definitions ...
end</code></pre></div></div></section><h2><a class="nav-anchor" id="Subproblem-definition-1" href="#Subproblem-definition-1">Subproblem definition</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Kokako.@stageobjective" href="#Kokako.@stageobjective"><code>Kokako.@stageobjective</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@stageobjective(subproblem, expr)</code></pre><p>Set the stage-objective of <code>subproblem</code> to <code>expr</code>.</p><p><strong>Example</strong></p><pre><code class="language-none">@stageobjective(subproblem, 2x + y)</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Kokako.parameterize" href="#Kokako.parameterize"><code>Kokako.parameterize</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">parameterize(modify::Function,
             subproblem::JuMP.Model,
             realizations::Vector{T},
             probability::Vector{Float64} = fill(1.0 / length(realizations))
                 ) where T</code></pre><p>Add a parameterization function <code>modify</code> to <code>subproblem</code>. The <code>modify</code> function takes one argument and modifies <code>subproblem</code> based on the realization of the noise sampled from <code>realizations</code> with corresponding probabilities <code>probability</code>.</p><p>In order to conduct an out-of-sample simulation, <code>modify</code> should accept arguments that are not in realizations (but still of type T).</p><p><strong>Example</strong></p><pre><code class="language-none">Kokako.parameterize(subproblem, [1, 2, 3], [0.4, 0.3, 0.3]) do ω
    JuMP.set_upper_bound(x, ω)
end</code></pre></div></div></section><h2><a class="nav-anchor" id="Training-the-policy-1" href="#Training-the-policy-1">Training the policy</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Kokako.train" href="#Kokako.train"><code>Kokako.train</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Kokako.train(graph::PolicyGraph; kwargs...)::TrainingResults</code></pre><p>Train the policy of the graph. Keyword arguments are</p><ul><li>iteration<em>limit: number of iterations to conduct before termination. Defaults to 100</em>000.</li><li>time_limit: number of seconds to train before termination. Defaults to Inf.</li><li>print_level: control the level of printing to the screen.</li><li>sampling_scheme: a sampling scheme to use on the forward pass of the algorithm. Defaults to InSampleMonteCarlo().</li></ul><p>There is also a special option for infinite horizon problems</p><ul><li>cycle<em>discretization</em>delta: the maximum distance between states allowed on the forward pass.</li></ul></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Kokako.termination_status" href="#Kokako.termination_status"><code>Kokako.termination_status</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">termination_status(results::TrainingResults)</code></pre><p>Query the reason why the training stopped.</p></div></div></section><h2><a class="nav-anchor" id="Simulating-the-policy-1" href="#Simulating-the-policy-1">Simulating the policy</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Kokako.simulate" href="#Kokako.simulate"><code>Kokako.simulate</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">simulate(graph::PolicyGraph,
         number_replications::Int = 1,
         variables::Vector{Symbol} = Symbol[];
         sampling_scheme::AbstractSamplingScheme =
             InSampleMonteCarlo(),
         custom_recorders = Dict{Symbol, Function}()
 )::Vector{Vector{Dict{Symbol, Any}}}</code></pre><p>Perform a simulation of the policy graph with <code>number_replications</code> replications using the sampling scheme <code>sampling_scheme</code>.</p><p>Returns a vector with one element for each replication. Each element is a vector with one-element for each node in the scenario that was sampled. Each element in that vector is a dictionary containing information about the subproblem that was solved.</p><p>In that dictionary there are four special keys:</p><ul><li>:node_index, which records the index of the sampled node in the policy graph</li><li>:noise_term, which records the noise observed at the node</li><li>:stage_objective, which records the stage-objective of the subproblem</li><li>:bellman_term, which records the cost/value-to-go of the node.</li></ul><p>The sum of :stage<em>objective + :bellman</em>term will equal the objective value of the solved subproblem.</p><p>In addition to the special keys, the dictionary will contain the result of <code>JuMP.value(subproblem[key])</code> for each <code>key</code> in <code>variables</code>. This is useful to obtain the primal value of the state and control variables.</p><p>For more complicated data, the <code>custom_recorders</code> keyword arguement can be used.</p><pre><code class="language-none">data = Dict{Symbol, Any}()
for (key, recorder) in custom_recorders
    data[key] = foo(subproblem)
end</code></pre><p>For example, to record the dual of a constraint named <code>my_constraint</code>, pass the following:</p><pre><code class="language-none">simulation_results = simulate(graph, number_replications=2;
    custom_recorders = Dict(
        :constraint_dual = (sp) -&gt; JuMP.dual(sp[:my_constraint])
    )
)</code></pre><p>The value of the dual in the first stage of the second replication can be accessed as:</p><pre><code class="language-none">simulation_results[2][1][:constraint_dual]</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Kokako.calculate_bound" href="#Kokako.calculate_bound"><code>Kokako.calculate_bound</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">Kokako.calculate_bound(graph::PolicyGraph, state::Dict{Symbol, Float64},
                       risk_measure=Expectation())</code></pre><p>Calculate the lower bound (if minimizing, otherwise upper bound) of the problem graph at the point state, assuming the risk measure at the root node is risk_measure.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Kokako.Historical" href="#Kokako.Historical"><code>Kokako.Historical</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Historical(scenarios::Vector{Vector{Tuple{T, S}}},
           probability::Vector{Float64})</code></pre><p>A sampling scheme that samples a scenario from the vector of scenarios <code>scenarios</code> according to <code>probability</code>. If probability omitted, defaults to uniform probability.</p><p><strong>Example</strong></p><pre><code class="language-none">Historical(
    [
        [(1, 0.5), (2, 1.0), (3, 0.5)],
        [(1, 0.5), (2, 0.0), (3, 1.0)],
        [(1, 1.0), (2, 0.0), (3, 0.0)]
    ],
    [0.2, 0.5, 0.3]
)</code></pre></div></div><div><div><pre><code class="language-none">Historical(scenario::Vector{Tuple{T, S}})</code></pre><p>A deterministic sampling scheme that always samples <code>scenario</code> with probability <code>1</code>.</p><p><strong>Example</strong></p><pre><code class="language-none">Historical([(1, 0.5), (2, 1.5), (3, 0.75)])</code></pre></div></div></section><footer><hr/><a class="previous" href="../tutorial/04_markov_uncertainty/"><span class="direction">Previous</span><span class="title">Tutorial Four: Markov uncertainty</span></a></footer></article></body></html>
